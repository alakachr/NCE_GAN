{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import exp\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from math import pi\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "from function_memoire import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.718281828459045"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mémoire NCE-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I ) NCE for a 1D distribution\n",
    "\n",
    "Data $X \\sim  N(m,s)$\n",
    "\n",
    "Noise $Y \\sim  Q = \\mu +\\sigma N(0,1)$ with $\\mu, \\sigma$ fixed (in the code it is fixed at mu_unit and sigma_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets experiment NCE with different values of $\\mu_{data}, \\sigma_{data} , \\mu_{noise}, \\sigma_{noise}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas 1 : Loi du bruit très distincte de l'échantillon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true constant value 0.3989422804014327\n",
      "fini à la  0  iteration\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Gradient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bc8028492631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNCEDescent1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmupo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmapo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcte_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnu\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"constant estimate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" nbre d'itérations\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/NCE_GAN/function_memoire.py\u001b[0m in \u001b[0;36mNCEDescent1D\u001b[0;34m(x_batches, m, s, mu_init, sigma_init, cte_init, learning_rate, max_iters, nu)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcte\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror_sigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_cte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigmas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Gradient' is not defined"
     ]
    }
   ],
   "source": [
    "mupo = 12\n",
    "sigmapo = 1\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size)) \n",
    "print(\"true constant value\" , 1/(sqrt(2*pi)*sigmapo))\n",
    "\n",
    "\n",
    "grad=NCEDescent1D(x_batches, mupo, sigmapo,mu_init = 24, sigma_init=4, cte_init = 5, learning_rate = [0.01,0.01], max_iters = 300, nu =1)    \n",
    "print(\"constant estimate\",grad.cte)\n",
    "print(\" nbre d'itérations\", len(grad.ctes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad=NCEDescent1D(x_batches, mupo, sigmapo,mu_init = 24, sigma_init=4, cte_init = 5, learning_rate = [0.01,0.01], max_iters = 100, nu =1)    \n",
    "print(\"constant estimate\",grad.cte)\n",
    "print(\"true constant value\" , 1/(sqrt(2*pi)*sigmapo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad=NCEDescent1D(x_batches, mupo, sigmapo,mu_init = 24, sigma_init=4, cte_init = 5, learning_rate = [1,0.01], max_iters = 500, nu =1)    \n",
    "print(\"constant estimate\",grad.cte)\n",
    "print(\"true constant value\" , 1/(sqrt(2*pi)*sigmapo))\n",
    "print(\" nbre d'itérations\", len(grad.ctes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarques :\n",
    "* L'algorithme n'effectue pas plus de 2 itérations lorsque bruit et échantillons sont trop similaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mupo = 2\n",
    "sigmapo = 0.2\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size)) \n",
    "\n",
    "\n",
    "x= np.linspace(1,100,num=50)\n",
    "\n",
    "ctes=[]\n",
    "V=[]\n",
    "C = []\n",
    "\n",
    "for i in range(len(x)):\n",
    "    \n",
    "    for j in range(10):\n",
    "        \n",
    "        grad = NCEDescent(x_batches,mupo, sigmapo,mu_init = 24, sigma_init=4, cte_init = 0.2, learning_rate = [0.01,0.01], max_iters = 100, nu =x[i])\n",
    "        #print(grad.m0)\n",
    "        ctes.append(grad.cte)\n",
    "    \n",
    "    C.append(np.mean(grad.cte))\n",
    "    V.append(np.var(ctes))\n",
    "    ctes = []\n",
    "    \n",
    "#print(ctes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mupo = 2\n",
    "sigmapo = 0.2\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size)) \n",
    "\n",
    "H = []\n",
    "\n",
    "for i in range (50):\n",
    "    grad = NCEDescente1D(x_batches,mupo, sigmapo,mu_init = 24, sigma_init=4, cte_init = 0.2, learning_rate = [0.01,0.01], max_iters = 500, nu =x[i])\n",
    "    H.append(grad.cte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.plot(x,V)\n",
    "#print(V)\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.scatter(x,C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarques:\n",
    " * Lorsque nu diverge (i.e la taille du bruit est relativement importante par rapport à celle de l'échantillon) le code semble se débloquer et le NCE a une variance chaotique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas 2: Loi du bruit\"raisonnablement\" distincte de celle de l'échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NCE \n",
    "\n",
    "mupo = 0.5\n",
    "sigmapo = 7\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size)) \n",
    "\n",
    "\n",
    "print(\"true constant value\" , 1/(sqrt(2*pi)*sigmapo))\n",
    "grad=NCE_Adam(x_batches,mupo, sigmapo,mu_init = 5, sigma_init=10, cte_init = 0.2, learning_rate = [0.01,0.01], max_iters = 500)    \n",
    "print(\"constant estimate\",grad.cte)\n",
    "\n",
    "print(\" nbre d'itérations\", len(grad.ctes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calcul variance NCE\n",
    "\n",
    "mupo = 0.5\n",
    "sigmapo = 0.2\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size)) \n",
    "\n",
    "\n",
    "x= np.linspace(1,100,num=50)\n",
    "\n",
    "ctes=[]\n",
    "C=[]\n",
    "V=[]\n",
    "for i in range(len(x)):\n",
    "    \n",
    "    for j in range(10):\n",
    "        \n",
    "        grad = NCEDescent(x_batches,mupo, sigmapo,mu_init = 0.5, sigma_init=1, cte_init = 0.2, learning_rate = [0.01,0.01], max_iters = 200, nu =x[i])\n",
    "        #print(grad.m0)\n",
    "        ctes.append(grad.cte)\n",
    "    \n",
    "    C.append(np.mean(grad.cte))\n",
    "    V.append(np.var(ctes))\n",
    "    ctes = []\n",
    "    \n",
    "#print(ctes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calcul histogramme NCE \n",
    "\n",
    "mupo = 0.5\n",
    "sigmapo = 0.2\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size))\n",
    "\n",
    "H = []\n",
    "\n",
    "for i in range (50):\n",
    "    grad = NCEDescent(x_batches,mupo, sigmapo,mu_init = 0.5, sigma_init= 1, cte_init = 3, learning_rate = [0.01,0.01], max_iters = 500, nu = 1)\n",
    "    H.append(grad.cte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.plot(x,V)\n",
    "#print(V)\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.scatter(x,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarques:\n",
    " * On remarque que conformément au résultat théorique. Lorsque nu diverge vers $+\\infty$ la variance du NCE diminue\n",
    " * L'estimateur ne semble pas biaisé d'après l'histogramme et variance relativement faible pour nu=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAS 3: Bruit Très similaire à l'échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NCE \n",
    "\n",
    "mupo = 4 #we take  mu_noise = 5\n",
    "sigmapo = 7\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size)) \n",
    "\n",
    "print(\"true constant value\" , 1/(sqrt(2*pi)*sigmapo))\n",
    "\n",
    "grad=NCE_Adam(x_batches,mupo, sigmapo,mu_init = 5, sigma_init=7, cte_init = 3, learning_rate = [0.01,0.01], max_iters = 1000)    \n",
    "print(\"constant estimate\",grad.cte)\n",
    "\n",
    "print( \" nombres d'iterations \", len(grad.ctes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NCE Variance en fonction de nu\n",
    "mupo = 0.5\n",
    "sigmapo = 1\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size)) \n",
    "\n",
    "\n",
    "x= np.linspace(1,100,num=50)\n",
    "\n",
    "ctes=[]\n",
    "C=[]\n",
    "V=[]\n",
    "\n",
    "for i in range(len(x)):\n",
    "    \n",
    "    for j in range(10):\n",
    "        \n",
    "        grad = NCEDescent(x_batches,mupo, sigmapo,mu_init = 5, sigma_init= 7, cte_init = 3, learning_rate = [0.01,0.01], max_iters = 200, nu =x[i])\n",
    "        #print(grad.m0)\n",
    "        ctes.append(grad.cte)\n",
    "    \n",
    "    C.append(np.mean(grad.cte))\n",
    "    V.append(np.var(ctes))\n",
    "    ctes = []\n",
    "    \n",
    "#print(ctes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Préparation Histogramme de la constante pour nu=1, m_data= 0.5, s_data = 1, m_noise=5, s_noise=7\n",
    "\n",
    "mupo = 0.5\n",
    "sigmapo = 1\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size))\n",
    "\n",
    "H = []\n",
    "\n",
    "for i in range (50):\n",
    "    grad = NCEDescent1D(x_batches,mupo, sigmapo,mu_init = 5, sigma_init= 7, cte_init = 3, learning_rate = [0.01,0.01], max_iters = 500, nu = 1)\n",
    "    H.append(grad.cte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot:\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "fig.add_subplot(1,3,1)\n",
    "plt.plot(x,V)\n",
    "#print(V)\n",
    "fig.add_subplot(1,3,2)\n",
    "plt.scatter(x,C)\n",
    "fig.add_subplot(1,3,3)\n",
    "plt.hist(H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarques: Loi du bruit modérément éloignée de la distribution initiale\n",
    " * On remarque que conformément au résultat théorique. Lorsque nu diverge vers $+\\infty$ la variance du NCE diminue\n",
    " * L'estimateur ne semble pas biaisé d'après l'histogramme et variance relativement faible pour nu=1.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN for estimation of a 1D density\n",
    "\n",
    "Generator is given by $G(z) = \\mu + \\sigma*z$ with$z ~ N(0,1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GANDescent1D(x_batches, m, s,mu_init , sigma_init, cte_init , learning_rate = [0.01,0.01], max_iters = 500, nu=1):    \n",
    "    \n",
    "    m0 = mu_init \n",
    "    s0 =sigma_init\n",
    "    cte = cte_init\n",
    "    \n",
    "    error_mu = [] \n",
    "    error_sigma = []\n",
    "    error_cte = [] \n",
    "    \n",
    "    mus = [m0]\n",
    "    sigmas = [s0]\n",
    "    ctes = [cte]\n",
    "\n",
    "    batch_size= len(x_batches[0])\n",
    "     \n",
    "    for itr in range(max_iters): \n",
    "        \n",
    "        for x in x_batches:\n",
    "            \n",
    "            z= random.normal( 0, 1, int(batch_size)) \n",
    "            q = m0+s0*z\n",
    "            \n",
    "            #Gradient in respect to the constant\n",
    "            grad_cte = np.sum( 1/cte - pm0(x,m,s)/(cte*pm0(x,m,s)+ pn(x, m0,s0)) )/batch_size - (1/ batch_size)*np.sum( pm0(q,m,s)/(cte*pm0(q,m,s)+ pn(q,m0,s0)) )\n",
    "            cte = cte + learning_rate[0] * grad_cte\n",
    "            error_cte.append( (grad_cte) ) \n",
    "            \n",
    "\n",
    "            #Gradient in respect to noise parameters\n",
    "           \n",
    "            grad_mu = -(q-m0)/s0**2 +((q-m0)*norm.pdf(q, m0, s0)/s0**2 + \n",
    "                        (q-m)*cte*exp(-0.5*((q-m)/s)**2)/s**2)/(cte*pm0(q,m,s) + norm.pdf(q,m0,s0))\n",
    "            grad_sigma = grad_mu*z\n",
    "            \n",
    "            grad_mu = np.sum(grad_mu)/batch_size\n",
    "            grad_sigma = np.sum(grad_sigma)/batch_size\n",
    "            \n",
    "            m0 = m0 - learning_rate[1] * grad_mu\n",
    "            s0 = s0 - learning_rate[1] * grad_sigma\n",
    "            \n",
    "            error_mu.append( (grad_mu) ) \n",
    "            error_sigma.append((grad_sigma))\n",
    "            \n",
    "            ctes.append(cte)\n",
    "            mus.append(m0)\n",
    "            sigmas.append(s0)\n",
    "            \n",
    "            if ( abs(ctes[-1] - ctes[-2]) < 1e-6 and abs(mus[-1]-mus[-2])<1e-6 and abs(sigmas[-1]-sigmas[-2])<1e-6):\n",
    "                return Gradient(cte,m0,s0, error_mu,error_sigma, error_cte, ctes,mus,sigmas)\n",
    "            \n",
    "  \n",
    "    result = Gradient(cte,m0,s0, error_mu,error_sigma, error_cte, ctes,mus,sigmas)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets experiment GAN with different values of  $𝜇_{𝑑𝑎𝑡𝑎},𝜎_{𝑑𝑎𝑡𝑎},𝜇_{𝑛𝑜𝑖𝑠𝑒},𝜎_{𝑛𝑜𝑖𝑠𝑒}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mupo = 24\n",
    "sigmapo = 7\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size)) \n",
    "\n",
    "print(\"#########  With good learning rate for gen parameters ########\")\n",
    "\n",
    "grad=GANDescent(x_batches,mupo, sigmapo,mu_init = 5, sigma_init=0.2, cte_init = 0.2, learning_rate = [0.01,0.1], max_iters = 500)    \n",
    "print(\"constant estimate\",grad.cte)\n",
    "print(\"true constant value\" , 1/(sqrt(2*pi)*sigmapo))\n",
    "print(\"mu generaor \",grad.mu)\n",
    "print(\"sigma generaor \",grad.sigma)\n",
    "\n",
    "print(\"\\n #########  With bad learning rate for gen parameters #######\")\n",
    "\n",
    "grad=GANDescent(x_batches,mupo, sigmapo,mu_init = 5, sigma_init=0.2, cte_init = 0.2, learning_rate = [0.01,0.01], max_iters = 500)    \n",
    "print(\"constant estimate\",grad.cte)\n",
    "print(\"true constant value\" , 1/(sqrt(2*pi)*sigmapo))\n",
    "print(\"mu generaor \",grad.mu)\n",
    "print(\"sigma generaor \",grad.sigma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mupo = 24\n",
    "sigmapo = 0.2\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size)) \n",
    "\n",
    "### Problemes numeriques !!!!\n",
    "\n",
    "grad=GANDescent(x_batches,mupo, sigmapo,mu_init = 5, sigma_init=1, cte_init = 0.2, learning_rate = [0.01,0.1], max_iters = 800)    \n",
    "print(\"constant estimate\",grad.cte)\n",
    "print(\"true constant value\" , 1/(sqrt(2*pi)*sigmapo))\n",
    "print(\"mu generaor \",grad.mu)\n",
    "print(\"sigma generaor \",grad.sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad.error_cte[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm0(mupo,mupo,sigmapo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mupo = 0.5\n",
    "sigmapo = 0.2\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size)) \n",
    "\n",
    "\n",
    "\n",
    "grad=GANDescent(x_batches,mupo, sigmapo,mu_init = 5, sigma_init=1, cte_init = 0.2, learning_rate = [0.01,0.1], max_iters = 800)    \n",
    "print(\"constant estimate\",grad.cte)\n",
    "print(\"true constant value\" , 1/(sqrt(2*pi)*sigmapo))\n",
    "print(\"mu generaor \",grad.mu)\n",
    "print(\"sigma generaor \",grad.sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots and  Numerical Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mupo = 5\n",
    "sigmapo = 0.2\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size)) \n",
    "\n",
    "l=50\n",
    "MU = np.linspace(-10 , 10, num=l)\n",
    "\n",
    "nces = np.ones(l)\n",
    "\n",
    "for i in range(l):\n",
    "    grad=GANDescent(x_batches,mupo, sigmapo,mu_init = MU[i], sigma_init= sigmapo, cte_init = 0.2, learning_rate = [0.01,0.1], max_iters = 500)    \n",
    "    nces[i]= grad.cte\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MU, nces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mupo = 5\n",
    "sigmapo = 0.2\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size)) \n",
    "\n",
    "l=200\n",
    "MU = np.linspace(-10 , 10, num=l)\n",
    "nces1 = np.ones(l)\n",
    "\n",
    "for i in range(l):\n",
    "    grad=NCEDescent(x_batches,mupo, sigmapo,mu_init = MU[i], sigma_init= sigmapo, cte_init = 0.2, learning_rate = [0.01,0.1], max_iters = 500)    \n",
    "\n",
    "    nces1[i]= grad.cte\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(MU, nces1)\n",
    "#plt.hist( nces1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mupo = 24\n",
    "sigmapo = 0.2\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size)) \n",
    "\n",
    "\n",
    "\n",
    "grad=NCEDescent1D(x_batches,mupo, sigmapo,mu_init = 24, sigma_init=0.2, cte_init = 0.2, learning_rate = 0.01, max_iters = 500)    \n",
    "print(\"constant estimate\",grad.cte)\n",
    "print(\"true constant value\" , 1/(sqrt(2*pi)*sigmapo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mupo = 0.5\n",
    "sigmapo = 7\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size)) \n",
    "\n",
    "\n",
    "\n",
    "grad=NCEDescent1D(x_batches,mupo, sigmapo,mu_init = 0.5, sigma_init=7, cte_init = 0.2, learning_rate = [0.01,0.01], max_iters = 500)    \n",
    "print(\"constant estimate\",grad.cte)\n",
    "print(\"true constant value\" , 1/(sqrt(2*pi)*sigmapo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mupo = 0.5 #we take  mu_noise = 5\n",
    "sigmapo = 7\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size)) \n",
    "\n",
    "\n",
    "\n",
    "grad=NCEDescent1D(x_batches,mupo, sigmapo,mu_init = 5, sigma_init=7, cte_init = 0.2, learning_rate = [0.01,0.01], max_iters = 500)    \n",
    "print(\"constant estimate\",grad.cte)\n",
    "print(\"true constant value\" , 1/(sqrt(2*pi)*sigmapo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mupo = 0.5 #we take  mu_noise = 5\n",
    "sigmapo = 7 #we take sigma_noise  = 5\n",
    "batch_size=100\n",
    "X = random.normal(mupo, sigmapo, 1000)\n",
    "x_batches = np.reshape(X, (10, batch_size)) \n",
    "\n",
    "\n",
    "\n",
    "grad=NCEDescent1D(x_batches,mupo, sigmapo,mu_init = 5, sigma_init=5, cte_init = 0.2, learning_rate = [0.01,0.01], max_iters = 500)    \n",
    "print(\"constant estimate\",grad.cte)\n",
    "print(\"true constant value\" , 1/(sqrt(2*pi)*sigmapo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cte = 1/(sqrt(2*pi)*0.2)\n",
    "cte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
