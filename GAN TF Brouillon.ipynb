{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f6e9f773c507>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#import tensorflow_probability as tfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named tensorflow"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_probability as tfp\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from  sklearn import mixture\n",
    "import scipy.sparse\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n",
    "tfd = tfp.distributions\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/romaric/.local/lib/python3.7/site-packages (3.2.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home/romaric/.local/lib/python3.7/site-packages (from matplotlib) (0.10.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/romaric/.local/lib/python3.7/site-packages (from matplotlib) (2.8.1)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.2.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/romaric/.local/lib/python3.7/site-packages (from matplotlib) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.18.2)\r\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (46.1.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --user matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnormalized Data Distribution\n",
    "def pm0(x, m, s):\n",
    "    mvn = tfd.MultivariateNormalFullCovariance(\n",
    "    loc=m,\n",
    "    covariance_matrix=s)\n",
    "    k = len(m)\n",
    "    return np.sqrt(np.linalg.det(s)*(2*pi)**k)*mvn.prob(x);\n",
    "\n",
    "# Noise Distribution\n",
    "def pn(x, m0, s0):\n",
    "    mvn = tfd.MultivariateNormalFullCovariance(\n",
    "    loc=m0,\n",
    "    covariance_matrix=s0)\n",
    "    \n",
    "    return mvn.prob(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This class is a helper that  will be instantiated in objects containing results from A Gradient Descent\n",
    "\n",
    "class Gradient:\n",
    "\n",
    "    # instance attribute\n",
    "    #Receive those attributes from\n",
    "    def __init__(self, cte,mu=[],sigma=[], error_mu =[],error_sigma=[], error_cte=[], ctes=[],mus=[],sigmas=[]):\n",
    "        self.cte = cte\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        self.error_mu = error_mu\n",
    "        self.error_sigma = error_sigma\n",
    "        self.error_cte = error_cte \n",
    "        \n",
    "        self.ctes = ctes\n",
    "        self.mus = mus\n",
    "        self.sigmas = sigmas\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_f2():\n",
    "    \n",
    "    g1= a*Z1 + b*Z2 + mu[0]\n",
    "    g2 = c*Z1 +  d*Z2 + mu[1]\n",
    "\n",
    "    g= tf.stack([g1,g2],1)\n",
    "\n",
    "\n",
    "    p_n = pn( g,m0, S) # noise (generator) density \n",
    "    p_m = pm0(g, m_data, s_data) #data density\n",
    "    return tf.math.reduce_mean( tf.math.log(p_n/(cte*p_m+p_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######## ALTERNATIVE VERSION #############\n",
    "\n",
    "# d =2\n",
    "m_data = [1,2]\n",
    "s_data = [[5, 11], [11, 25]]\n",
    "\n",
    "####Cholesky gives : [1,] [3,4]\n",
    "\n",
    "batch_size=100\n",
    "x_batches = multivariate_normal.rvs(m_data, s_data, 10,batch_size)\n",
    "\n",
    "\n",
    "mu_init = [5, -3]\n",
    "a,b,c,d =  tf.Variable((-5+ 1), dtype=\"float32\"), tf.Variable(8+2, dtype=\"float32\"), tf.Variable(5+3, dtype=\"float32\"), tf.Variable(1+ 4 ,dtype=\"float32\")\n",
    "cte_init = 1\n",
    "\n",
    "mu = tf.Variable(mu_init, dtype=\"float32\")\n",
    "\n",
    "cte = cte_init\n",
    "error_mu = [] \n",
    "error_sigma = []\n",
    "\n",
    "\n",
    "error_cte = [] \n",
    "\n",
    "mus = []\n",
    "sigmas = []\n",
    "\n",
    "\n",
    "ctes = []\n",
    "max_iters = 700\n",
    "learning_rate = 0.01\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    " \n",
    "for itr in range(max_iters): \n",
    "    #print(itr)\n",
    "    for x in x_batches:\n",
    "        ctes.append(cte)\n",
    "        Z1 = np.array(random.normal( 0, 1,batch_size)).astype(\"float32\")\n",
    "        Z2 = np.array(random.normal( 0, 1,batch_size)).astype(\"float32\")\n",
    "        \n",
    "        g1= a*Z1 + b*Z2 + mu[0]\n",
    "        g2 = c*Z1 +  d*Z2 + mu[1]\n",
    "\n",
    "        A = np.array([[a,b],[c,d]])\n",
    "        S = np.array(np.matmul(A , np.transpose(A))).astype(\"float32\")\n",
    "        \n",
    "       \n",
    "\n",
    "        g= np.stack([g1,g2],1)\n",
    "        \n",
    "       \n",
    "        \n",
    "        m0 = mu.numpy()\n",
    "        \n",
    "        p_nx = pn( x,m0, S).numpy() # noise (generator) density \n",
    "        p_mx = pm0(x, m_data, s_data).numpy() #data density\n",
    "\n",
    "        \n",
    "        p_ng = pn( g,m0, S).numpy() # noise (generator) density \n",
    "        p_mg = pm0(g, m_data, s_data).numpy() #data density\n",
    "\n",
    "        grad_cte = 1/cte - p_mx/(cte*p_mx+p_nx) -  p_mg/(cte*p_mg+p_ng)\n",
    "        grad_cte = np.sum(grad_cte)/batch_size\n",
    "            \n",
    "        \n",
    "        cte = cte + 0.01*grad_cte\n",
    "        \n",
    "#         print(\"gradient   \" , grad_cte )\n",
    "#         print(\"cte\" , cte)\n",
    "        error_cte.append( (grad_cte) ) \n",
    "        if np.isnan(grad_cte):\n",
    "            break\n",
    "            \n",
    "        opt.minimize(obj_f2, var_list=[a,b,c,d, mu])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cte)\n",
    "print(1/np.sqrt(np.linalg.det(s_data)*(2*pi)**len(m_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[a.numpy(),b.numpy()],[c.numpy(),d.numpy()]]\n",
    "\n",
    "print(A)\n",
    "\n",
    "print(np.matmul(A, np.transpose(A)))\n",
    "\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GANDescent(x_batches, m, s,mu_init , A_init, cte_init , opt, rate = 0.01, max_iters = 500):    \n",
    "    # d =2\n",
    "    m_data = m\n",
    "    s_data = s\n",
    "\n",
    "    \n",
    "    a,b =  tf.Variable(A_init[0][0], dtype=\"float32\"), tf.Variable(A_init[0][1], dtype=\"float32\")\n",
    "    c,d = tf.Variable(A_init[1][0], dtype=\"float32\"), tf.Variable(A_init[1][1] ,dtype=\"float32\")\n",
    "\n",
    "\n",
    "    mu = tf.Variable(mu_init, dtype=\"float32\")\n",
    "\n",
    "    cte = cte_init\n",
    "    error_mu = [] \n",
    "    error_sigma = []\n",
    "    error_cte = [] \n",
    "    mus = []\n",
    "    sigmas = []\n",
    "    ctes = []\n",
    "\n",
    "    batch_size =len(x_batches[0])\n",
    "    #opt = tf.keras.optimizers.SGD(learning_rate=rate[1])\n",
    "    \n",
    "    def obj_f2():\n",
    "    \n",
    "        g1= a*Z1 + b*Z2 + mu[0]\n",
    "        g2 = c*Z1 +  d*Z2 + mu[1]\n",
    "\n",
    "        g= tf.stack([g1,g2],1)\n",
    "\n",
    "\n",
    "        p_n = pn( g,m0, S) # noise (generator) density \n",
    "        p_m = pm0(g, m_data, s_data) #data density\n",
    "        return tf.math.reduce_mean( tf.math.log(p_n/(cte*p_m+p_n)))\n",
    "\n",
    "    for itr in range(max_iters): \n",
    "        #print(itr)\n",
    "        for x in x_batches:\n",
    "            ctes.append(cte)\n",
    "            Z1 = np.array(random.normal( 0, 1,batch_size)).astype(\"float32\")\n",
    "            Z2 = np.array(random.normal( 0, 1,batch_size)).astype(\"float32\")\n",
    "\n",
    "            g1= a*Z1 + b*Z2 + mu[0]\n",
    "            g2 = c*Z1 +  d*Z2 + mu[1]\n",
    "\n",
    "            A = np.array([[a,b],[c,d]])\n",
    "            S = np.array(np.matmul(A , np.transpose(A))).astype(\"float32\")\n",
    "\n",
    "            g= np.stack([g1,g2],1)\n",
    "\n",
    "            m0 = mu.numpy()\n",
    "\n",
    "            p_nx = pn( x,m0, S).numpy() # noise (generator) density \n",
    "            p_mx = pm0(x, m_data, s_data).numpy() #data density\n",
    "\n",
    "\n",
    "            p_ng = pn( g,m0, S).numpy() # noise (generator) density \n",
    "            p_mg = pm0(g, m_data, s_data).numpy() #data density\n",
    "\n",
    "            grad_cte = 1/cte - p_mx/(cte*p_mx+p_nx) -  p_mg/(cte*p_mg+p_ng)\n",
    "            grad_cte = np.sum(grad_cte)/batch_size\n",
    "\n",
    "\n",
    "            cte = cte + rate*grad_cte\n",
    "\n",
    "            if np.isnan(grad_cte):\n",
    "                print(\"WARNING : NAN values\")\n",
    "                break\n",
    "\n",
    "            opt.minimize(obj_f2, var_list=[ mu])\n",
    "\n",
    "    result = Gradient(cte,mu.numpy(),[[a.numpy(),b.numpy()],[c.numpy(),d.numpy()]], error_cte=error_cte, ctes = ctes)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d =2\n",
    "m_data = [1,2]\n",
    "s_data = [[5, 11], [11, 25]]\n",
    "\n",
    "batch_size=100\n",
    "x_batches = multivariate_normal.rvs(m_data, s_data, 10,batch_size)\n",
    "mu_init = [9, 9]\n",
    "a,b,c,d = 1, 2, 3, 4\n",
    "cte_init = 1\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "grad=GANDescent(x_batches, m_data, s_data,mu_init , [[a,b],[c,d]], cte_init , opt, rate = 0.01, max_iters = 500)\n",
    "\n",
    "print(\"constant estimate\",grad.cte)\n",
    "print(\"true constant value\" , 1/np.sqrt(np.linalg.det(s_data)*(2*pi)**len(m_data)))\n",
    "print(\"mu generaor \",grad.mu)\n",
    "A = grad.sigma\n",
    "print(\"A gen\", A)\n",
    "\n",
    "print(\" SIGMA = AA'\", np.matmul(A, np.transpose(A)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.Variable(10.0, dtype =\"double\")\n",
    "x1, x2 = reset()\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "for i in range(50):\n",
    "\tprint ('y = {:.1f}, x1 = {:.1f}, x2 = {:.1f}'.format(fu(x1, x2).numpy(), x1.numpy(), x2.numpy()))\n",
    "\topt.minimize(fu_minimzie, var_list=[x1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
