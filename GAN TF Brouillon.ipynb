{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from  sklearn import mixture\n",
    "import scipy.sparse\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n",
    "tfd = tfp.distributions\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnormalized Data Distribution\n",
    "def pm0(x, m, s):\n",
    "    mvn = tfd.MultivariateNormalFullCovariance(\n",
    "    loc=m,\n",
    "    covariance_matrix=s)\n",
    "    k = len(m)\n",
    "    return np.sqrt(np.linalg.det(s)*(2*pi)**k)*mvn.prob(x);\n",
    "\n",
    "# Noise Distribution\n",
    "def pn(x, m0, s0):\n",
    "    mvn = tfd.MultivariateNormalFullCovariance(\n",
    "    loc=m0,\n",
    "    covariance_matrix=s0)\n",
    "    \n",
    "    return mvn.prob(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This class is a helper that  will be instantiated in objects containing results from A Gradient Descent\n",
    "\n",
    "class Gradient:\n",
    "\n",
    "    # instance attribute\n",
    "    #Receive those attributes from\n",
    "    def __init__(self, cte,mu=[],sigma=[], error_mu =[],error_sigma=[], error_cte=[], ctes=[],mus=[],sigmas=[]):\n",
    "        self.cte = cte\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        self.error_mu = error_mu\n",
    "        self.error_sigma = error_sigma\n",
    "        self.error_cte = error_cte \n",
    "        \n",
    "        self.ctes = ctes\n",
    "        self.mus = mus\n",
    "        self.sigmas = sigmas\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_f2():\n",
    "    \n",
    "    g1= a*Z1 + b*Z2 + mu[0]\n",
    "    g2 = c*Z1 +  d*Z2 + mu[1]\n",
    "\n",
    "    g= tf.stack([g1,g2],1)\n",
    "\n",
    "\n",
    "    p_n = pn( g,m0, S) # noise (generator) density \n",
    "    p_m = pm0(g, m_data, s_data) #data density\n",
    "    return tf.math.reduce_mean( tf.math.log(p_n/(cte*p_m+p_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######## ALTERNATIVE VERSION #############\n",
    "\n",
    "# d =2\n",
    "m_data = [1,2]\n",
    "s_data = [[5, 11], [11, 25]]\n",
    "\n",
    "####Cholesky gives : [1,] [3,4]\n",
    "\n",
    "batch_size=100\n",
    "x_batches = multivariate_normal.rvs(m_data, s_data, 10,batch_size)\n",
    "\n",
    "\n",
    "mu_init = [5, -3]\n",
    "a,b,c,d =  tf.Variable((-5+ 1), dtype=\"float32\"), tf.Variable(8+2, dtype=\"float32\"), tf.Variable(5+3, dtype=\"float32\"), tf.Variable(1+ 4 ,dtype=\"float32\")\n",
    "cte_init = 1\n",
    "\n",
    "mu = tf.Variable(mu_init, dtype=\"float32\")\n",
    "\n",
    "cte = cte_init\n",
    "error_mu = [] \n",
    "error_sigma = []\n",
    "\n",
    "\n",
    "error_cte = [] \n",
    "\n",
    "mus = []\n",
    "sigmas = []\n",
    "\n",
    "\n",
    "ctes = []\n",
    "max_iters = 700\n",
    "learning_rate = 0.01\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    " \n",
    "for itr in range(max_iters): \n",
    "    #print(itr)\n",
    "    for x in x_batches:\n",
    "        ctes.append(cte)\n",
    "        Z1 = np.array(random.normal( 0, 1,batch_size)).astype(\"float32\")\n",
    "        Z2 = np.array(random.normal( 0, 1,batch_size)).astype(\"float32\")\n",
    "        \n",
    "        g1= a*Z1 + b*Z2 + mu[0]\n",
    "        g2 = c*Z1 +  d*Z2 + mu[1]\n",
    "\n",
    "        A = np.array([[a,b],[c,d]])\n",
    "        S = np.array(np.matmul(A , np.transpose(A))).astype(\"float32\")\n",
    "        \n",
    "       \n",
    "\n",
    "        g= np.stack([g1,g2],1)\n",
    "        \n",
    "       \n",
    "        \n",
    "        m0 = mu.numpy()\n",
    "        \n",
    "        p_nx = pn( x,m0, S).numpy() # noise (generator) density \n",
    "        p_mx = pm0(x, m_data, s_data).numpy() #data density\n",
    "\n",
    "        \n",
    "        p_ng = pn( g,m0, S).numpy() # noise (generator) density \n",
    "        p_mg = pm0(g, m_data, s_data).numpy() #data density\n",
    "\n",
    "        grad_cte = 1/cte - p_mx/(cte*p_mx+p_nx) -  p_mg/(cte*p_mg+p_ng)\n",
    "        grad_cte = np.sum(grad_cte)/batch_size\n",
    "            \n",
    "        \n",
    "        cte = cte + 0.01*grad_cte\n",
    "        \n",
    "#         print(\"gradient   \" , grad_cte )\n",
    "#         print(\"cte\" , cte)\n",
    "        error_cte.append( (grad_cte) ) \n",
    "        if np.isnan(grad_cte):\n",
    "            break\n",
    "            \n",
    "        opt.minimize(obj_f2, var_list=[a,b,c,d, mu])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07957778655290482\n",
      "0.0795774715459475\n"
     ]
    }
   ],
   "source": [
    "print(cte)\n",
    "print(1/np.sqrt(np.linalg.det(s_data)*(2*pi)**len(m_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18164639, 2.2286787], [1.2910917, 4.830433]]\n",
      "[[ 5.0000043 11.000005 ]\n",
      " [11.000005  25.       ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([1.0000005, 1.9999996], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [[a.numpy(),b.numpy()],[c.numpy(),d.numpy()]]\n",
    "\n",
    "print(A)\n",
    "\n",
    "print(np.matmul(A, np.transpose(A)))\n",
    "\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GANDescent(x_batches, m, s,mu_init , A_init, cte_init , opt, rate = 0.01, max_iters = 500):    \n",
    "    # d =2\n",
    "    m_data = m\n",
    "    s_data = s\n",
    "\n",
    "    \n",
    "    a,b =  tf.Variable(A_init[0][0], dtype=\"float32\"), tf.Variable(A_init[0][1], dtype=\"float32\")\n",
    "    c,d = tf.Variable(A_init[1][0], dtype=\"float32\"), tf.Variable(A_init[1][1] ,dtype=\"float32\")\n",
    "\n",
    "\n",
    "    mu = tf.Variable(mu_init, dtype=\"float32\")\n",
    "\n",
    "    cte = cte_init\n",
    "    error_mu = [] \n",
    "    error_sigma = []\n",
    "    error_cte = [] \n",
    "    mus = []\n",
    "    sigmas = []\n",
    "    ctes = []\n",
    "\n",
    "    batch_size =len(x_batches[0])\n",
    "    #opt = tf.keras.optimizers.SGD(learning_rate=rate[1])\n",
    "    \n",
    "    def obj_f2():\n",
    "    \n",
    "        g1= a*Z1 + b*Z2 + mu[0]\n",
    "        g2 = c*Z1 +  d*Z2 + mu[1]\n",
    "\n",
    "        g= tf.stack([g1,g2],1)\n",
    "\n",
    "\n",
    "        p_n = pn( g,m0, S) # noise (generator) density \n",
    "        p_m = pm0(g, m_data, s_data) #data density\n",
    "        return tf.math.reduce_mean( tf.math.log(p_n/(cte*p_m+p_n)))\n",
    "\n",
    "    for itr in range(max_iters): \n",
    "        #print(itr)\n",
    "        for x in x_batches:\n",
    "            ctes.append(cte)\n",
    "            Z1 = np.array(random.normal( 0, 1,batch_size)).astype(\"float32\")\n",
    "            Z2 = np.array(random.normal( 0, 1,batch_size)).astype(\"float32\")\n",
    "\n",
    "            g1= a*Z1 + b*Z2 + mu[0]\n",
    "            g2 = c*Z1 +  d*Z2 + mu[1]\n",
    "\n",
    "            A = np.array([[a,b],[c,d]])\n",
    "            S = np.array(np.matmul(A , np.transpose(A))).astype(\"float32\")\n",
    "\n",
    "            g= np.stack([g1,g2],1)\n",
    "\n",
    "            m0 = mu.numpy()\n",
    "\n",
    "            p_nx = pn( x,m0, S).numpy() # noise (generator) density \n",
    "            p_mx = pm0(x, m_data, s_data).numpy() #data density\n",
    "\n",
    "\n",
    "            p_ng = pn( g,m0, S).numpy() # noise (generator) density \n",
    "            p_mg = pm0(g, m_data, s_data).numpy() #data density\n",
    "\n",
    "            grad_cte = 1/cte - p_mx/(cte*p_mx+p_nx) -  p_mg/(cte*p_mg+p_ng)\n",
    "            grad_cte = np.sum(grad_cte)/batch_size\n",
    "\n",
    "\n",
    "            cte = cte + rate*grad_cte\n",
    "\n",
    "            if np.isnan(grad_cte):\n",
    "                print(\"WARNING : NAN values\")\n",
    "                break\n",
    "\n",
    "            opt.minimize(obj_f2, var_list=[ mu])\n",
    "\n",
    "    result = Gradient(cte,mu.numpy(),[[a.numpy(),b.numpy()],[c.numpy(),d.numpy()]], error_cte=error_cte, ctes = ctes)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d =2\n",
    "m_data = [1,2]\n",
    "s_data = [[5, 11], [11, 25]]\n",
    "\n",
    "batch_size=100\n",
    "x_batches = multivariate_normal.rvs(m_data, s_data, 10,batch_size)\n",
    "mu_init = [9, 9]\n",
    "a,b,c,d = 1, 2, 3, 4\n",
    "cte_init = 1\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "grad=GANDescent(x_batches, m_data, s_data,mu_init , [[a,b],[c,d]], cte_init , opt, rate = 0.01, max_iters = 500)\n",
    "\n",
    "print(\"constant estimate\",grad.cte)\n",
    "print(\"true constant value\" , 1/np.sqrt(np.linalg.det(s_data)*(2*pi)**len(m_data)))\n",
    "print(\"mu generaor \",grad.mu)\n",
    "A = grad.sigma\n",
    "print(\"A gen\", A)\n",
    "\n",
    "print(\" SIGMA = AA'\", np.matmul(A, np.transpose(A)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.Variable(10.0, dtype =\"double\")\n",
    "x1, x2 = reset()\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "for i in range(50):\n",
    "\tprint ('y = {:.1f}, x1 = {:.1f}, x2 = {:.1f}'.format(fu(x1, x2).numpy(), x1.numpy(), x2.numpy()))\n",
    "\topt.minimize(fu_minimzie, var_list=[x1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
